{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Matrix Factorization \n",
    "## Introduction\n",
    "In this notebook, we implement probabilistic matrix factorization (PMF) on movie rating dataset.\n",
    "\n",
    "Matrix factorization is a method to decompose a matrix into multiplication of matrices (usually two matrices). A matrix can represent a dyadic data. Matrix factorization is a way to better understand the components (dyads), and use them in further tasks. The goal is to find those unknown matrices.\n",
    "\n",
    "Probabilistic matrix factorization (PMF) is a probabilistic extension of matrix factorization where unknown parameters are described with their probability distributions. Similar to matrix factorization, the goal of PMF is to learn embeddings from dyadic/relational data (each matrix entry is a dyad, e.g., user-item rating, document-word count, user-user link). If the matrix has missing values, by learning decomposition based on observed entries, missing values can be pre dicted. This technique is useful in variety of applications such as recommender systems/collaborative filtering, link prediction in social network, and so on. In this project, are going to use PMF in collaborating filtering. The goal of collab orative filtering is to infer user preferences for items given a large but incomplete collection of preferences for many users. If we let each row of the original matrix represent a user, and each column represent a product, we can interpret the entries of the matrix to be the rating given to a product by a user.\n",
    "\n",
    "By learning latent features for users and movies using the seen ratings, we are able to predict the ratings for the unseen entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model \n",
    "For more clarification, we show the model in the context of movie ratings. Let $R$ be the matrix of ratings which is partially (and sparsely) filled by users' ratings for movies. Let us assume that there are $N$ users and $M$ movies in total. The goal is to find a decomposition for $R$ such that:\n",
    "$$R \\approx U^T V$$\n",
    "where $U$ is a $d\\times N$ matrix describing users and $V$ is a $d\\times M$ matrix describing movies. \n",
    "\n",
    "Figure below shows an illustration of this model where each column of $U$ and $V$ learn a representation for the users and movies respectively.\n",
    "\n",
    "<img src=\"matrix_fact.png\",style=\"max-width:100%; width: 50%\">\n",
    "\n",
    "\n",
    "Given feature vectors for $U_i$ user and $V_j$, the distribution of the corresponding rating is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R(i,j) \\sim N(U_i^TV_j , \\sigma^2)$$\n",
    "$$U_i \\sim N(0, \\sigma_u^2 I) ~~~~ V_j \\sim N(0, \\sigma_v^2 I)$$\n",
    "\n",
    "The graphical model of it is:\n",
    "\n",
    "<img src=\"graph-model.png\",height: 20>\n",
    "\n",
    "Our objective is to find $U$ and $V$ such that they have the maximum probabillity to generate seen ratings $R$. This method is called Maximum a Posteriori (MAP) estimation. Note that $\\lambda$ is a tuning parameter resulting in more stability. Let $D$ represent set of all observed data such that: $D~: ~\\{(i,j) ~| ~R(i,j)~is ~given\\}$. Then the objective is:\n",
    "\n",
    "$$\\underset{U,V}{\\max} ~ \\log p(U,V|R) $$\n",
    "$$\\underset{U,V}{\\max} ~ L(U,V)=-\\frac{1}{2\\sigma^2} \\sum_{(i,j)\\in D} {(R_{ij} - U_i^{\\intercal} V_j)}^2 - \\frac{\\lambda}{2} \\sum_{i=1}^{M} {||U_i||}^2 - \\frac{\\lambda}{2} \\sum_{j=1}^{N} {||V_j||}^2$$\n",
    "\n",
    "The optimization algorithm is as follows:\n",
    "\n",
    "- Initialize $U^{(0)}$ and $V^{(0)}$\n",
    "- For $t = 1:T$\n",
    "    - $U^{(t)}_i = {\\left[ \\lambda I + \\sum_{j:(i,j)\\in D} \\frac{1}{\\sigma^2} {V^{(t-1)}_j}^{\\intercal}V^{(t-1)}_j  \\right]}^{-1} \\left(\\sum_{j:(i,j)\\in D} \\frac{1}{\\sigma^2}R_{ij} ~ V^{(t-1)}_j \\right)$\n",
    "    - $V^{(t)}_j = {\\left[ \\lambda I + \\sum_{i:(i,j)\\in D} \\frac{1}{\\sigma^2} {U^{(t)}_i}^{\\intercal} U^{(t)}_i  \\right]}^{-1} \\left(\\sum_{i:(i,j)\\in D} \\frac{1}{\\sigma^2}R_{ij} ~ U^{(t)}_i \\right)\t\n",
    "$\n",
    "\n",
    "    - End for.\n",
    "    \n",
    "    \n",
    "Now, the first step is to import the data.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to load your data, and put it in a matrix called $R$. Make sure to convert the ratings into floats - not categorical variables. The goal for you is to complete the matrix and find the missing entries. To make calculations easier, create a new matrix with 3 columns. Put the row indices (user ids) in the first row, and column indices (movie ids) in second column, and let the third column show the ratings.\n",
    "Here is an implementation of it for MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load your data here...\n",
    "# example with MovieLens data:\n",
    "from numpy import *\n",
    "import random\n",
    "import pandas as pd\n",
    "prefer = []\n",
    "for line in open('data/u.data.txt', 'r'):  \n",
    "    (userid, movieid, rating, ts) = line.split('\\t')  \n",
    "    uid = int(userid)\n",
    "    mid = int(movieid)\n",
    "    rat = float(rating)\n",
    "    prefer.append([uid, mid, rat])\n",
    "    #data = array(prefer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are 943 users and 1682 movies\\objects.\n"
     ]
    }
   ],
   "source": [
    "data = array(prefer)\n",
    "N = len(np.unique(data[:,0])) # number of users\n",
    "M = len(np.unique(data[:,1])) # number of movies\n",
    "print(\"In this dataset, there are {} users and {} movies\\objects.\".format(N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 100000 given ratings total.\n",
      "Here is the first 10 rows in my processesed data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 196.,  242.,    3.],\n",
       "       [ 186.,  302.,    3.],\n",
       "       [  22.,  377.,    1.],\n",
       "       [ 244.,   51.,    2.],\n",
       "       [ 166.,  346.,    1.],\n",
       "       [ 298.,  474.,    4.],\n",
       "       [ 115.,  265.,    2.],\n",
       "       [ 253.,  465.,    5.],\n",
       "       [ 305.,  451.,    3.],\n",
       "       [   6.,   86.,    3.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"there are {} given ratings total.\".format(len(data)))\n",
    "      \n",
    "print(\"Here is the first 10 rows in my processesed data\")\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to run your algorithm. Note that at each sep, you need to update all of the movies' features as well as all of the users' features. You can set lambda = 0.1, and variances to be 1. Note that these parameter choices are up to you, and feel free to tune them the way you want. You can set the number of iterations to be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize U and V randomly. Note that U should be d by N and V should be d by M. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the calculations easier, you should keep track of some values at each step. Note that to update vector for a user, you need to have access to all the movies rated by that user (the same for the movies; to update vector for a movie, you need the id of all of the users that have rated that movie.)\n",
    "Since these don't change, we suggest you to construct a dictionary for each user as key, and id of movies rated by that user to be the values (as a list). You should do the same for movies.\n",
    "These are only some suggestions, feel free to use any method that you like. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can construct two dictionaries one for the user and one for the movies here.\n",
    "## hint: you can use defaultdict\n",
    "\n",
    "# from collections import defaultdict\n",
    "#\n",
    "# u_list = defaultdict(list)\n",
    "# for i, j in zip(data[:,0],data[:,0]):\n",
    "#    u_list[i].append(j)\n",
    "#    \n",
    "# m_list = defaultdict(list)\n",
    "# for i, j in zip(data[:,0],data[:,1]):\n",
    "#    m_list[i].append(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to update feature vector of a user, you need to compute ${V^{(t-1)}_j}^{\\intercal}V^{(t-1)}_j$. We suggest you to ceate an array for all of the movies, and update these values at the end of each iteration. That way, you just need to look up the dictionary for the movie ids that a user has rated, and use those indices of your constructed vector.\n",
    "You need to do the same for the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Constructing arrays that will make your life easier in the implementation of the algorithm ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Implementation of the algorithm ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you found the optimal U and V matrices, you can predict for the missing ratings for users. For example, show 10 movies with the highest predicted ratings for 5 of the users of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## e.g.: If you were a movie recommending system, what would you recommend to user number 19 to watch that hasn't watched before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "1- Mnih, A., & Salakhutdinov, R. (2007). Probabilistic matrix factorization. In Advances in neural information processing systems (pp. 1257-1264). https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
